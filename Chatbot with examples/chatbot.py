# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XAmNhjIbryGLkagzXVXsunNZSxny3lPg
"""

# Import necessary libraries
import tensorflow as tf
import numpy as np
import json
import regex as re
from google.colab import drive
import nltk
from nltk.tokenize import word_tokenize
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from keras.layers import Embedding, LSTM, Dense, Dropout,Input

#Mounting to drive where dialogues are loaded
drive.mount('/content/drive')


# Change to the directory where the folder is located
import os
os.chdir('/content/drive/My Drive/dialogues')

# Use the glob module to get a list of all the txt files in the folder
import glob
txt_files = glob.glob('*.txt')

#Storing all dialogues in a list. Our dialogues are in json format so, in order 
#to read them we have to use json library, and then extract all availabe dialogues ('turns')
dialogue_list = [] 
for txt_file in txt_files:
  with open(txt_file) as f:
    for jsonObj in f:
      dialogue_dict=json.loads(jsonObj)
      dialogue_list.append(dialogue_dict['turns'])

#Our goal here is to create two lists. One containing the questions and the other containing the answers. We notice that every turn starts with the sentence 'Hello, how may i help you', 
# a line clearly produced from bots. So, in order to achive the format we want, we append a random greeting, and we suppose the conversation starts with one of the greetings.
# Then, we have to make sure that every dialogue ends with a sentence produced by the bot. So, in order to achive this, in the ending of every dialogue, we append a random word from 
# 'beys' list.  
import random
greetings=['hello','hi','hey']
byes=['bye','seeya','goodbye']
questions=[]
answers=[]
for dialogue in dialogue_list:
  questions.append(random.choice(greetings))
  #create a boolean variable in order to know who is talking, the user or the bot.
  bot_talking=True
  for talking in dialogue:
    if bot_talking==True:
      answers.append(talking)
      bot_talking=False
      continue
    else:
      questions.append(talking)
      bot_talking=True
      continue
  if bot_talking==True:
    answers.append(random.choice(byes))

#In order to preproccess every questions and answers we have to replace
# all of the abbreviations with their full form (i.e. replace 'i'm' with 'i am') 
#and remove all special symbols such as ! and ?
def preprocess_text(text):
  text = re.sub(r"i'm","i am", text.lower())
  text = re.sub(r"he's","he is", text.lower())
  text = re.sub(r"she's","she is", text.lower())
  text = re.sub(r"it's","it is", text.lower())
  text = re.sub(r"that's","that is", text.lower())
  text = re.sub(r"what's","what is", text.lower())
  text = re.sub(r"where's","where is", text.lower())
  text = re.sub(r"how's","how is", text.lower())
  text = re.sub(r"\'re"," are", text.lower())
  text = re.sub(r"\'ll"," will", text.lower())
  text = re.sub(r"\'ve"," have", text.lower())
  text = re.sub(r"\'d"," would", text.lower())
  text = re.sub(r"won't","will not", text.lower())
  text = re.sub(r"can't","can not", text.lower())
  text = re.sub(r"n't"," not", text.lower())
  text = re.sub(r"[,.?/'-+:=)(*&%$#@!)~<>`]"," ", text)
  return text

#We pass every question and every answer in our function
i=0
for line in questions:
  questions[i]=preprocess_text(line)
  i+=1

i=0
for line in answers:
  answers[i]=preprocess_text(line)
  i+=1

# In order to reduce the computational cost of our training, we keep only the questions and answers that has 15 words maximum. 
short_questions=[]
short_answers=[]
for i in range(len(questions)):
  if len(questions[i].split())<=15:
    short_questions.append(questions[i])
    short_answers.append(answers[i])
short_questions_new=[]
short_answers_new=[]
for i in range(len(short_answers)):
  if len(short_answers[i].split())<=15:
    short_questions_new.append(short_questions[i])
    short_answers_new.append(short_answers[i])

# To evaluate our chatbot we take a subset of our short answers 
evaluating_answers=[]
for i in range(12000,17000):
  evaluating_answers.append(short_answers_new[i])

# We have to define a token for Start Of Sentence and a token for End Of Sentence
for i in range(len(short_answers_new)):
  short_answers_new[i]= '<SOS> ' + short_answers_new[i] + ' <EOS>'
  short_questions_new[i]= '<SOS> ' + short_questions_new[i] + ' <EOS>'

# We can see that we have 209.497 total questions and answers to train on 
len(short_answers_new)

evaluating_questions=[]
for i in range(12000,17000):
  evaluating_questions.append(short_questions_new[i])

## We are creating a vocabulary using our short questions and answers.
tokenizer = Tokenizer(num_words=14999)
tokenizer.fit_on_texts(short_questions_new + short_answers_new)
dictionary=tokenizer.word_index

#Creating 2 dictionaries that will help us later. One dictionary stores the index of every word used , and the other dixtionary stores the word of every index of our vocabulary.
word_to_index={}
index_to_word={}
for k, v in dictionary.items():
      if v < 14999:
          word_to_index[k] = v
          index_to_word[v] = k
      if v >= 14999:
          continue

# And now we convert every text to a numerical vector with its number of elements
# being equal to word count. Every word is converted into a number using the vocabulary our tokenizer created
encoder_sequences=tokenizer.texts_to_sequences(short_questions_new)
decoder_sequences=tokenizer.texts_to_sequences(short_answers_new)
# We want all of our vectors to have the same length, so we use the pad_sequences method to convert
# every vector to a new one with length = 15(max number of words in our sentences) + 2(one for <SOS> and one for <EOS>)
encoder_input_data=pad_sequences(encoder_sequences,maxlen=17,padding='post',truncating='post')
decoder_input_data=pad_sequences(decoder_sequences,maxlen=17,padding='post',truncating='post')

#For the decoder output we have to ignore the token <SOS> 
decoder_final_output = []
for i in decoder_input_data:
    decoder_final_output.append(i[1:]) 

decoder_final_output = pad_sequences(decoder_final_output, 17, padding='post', truncating='post')

"""# TIME FOR SEQ2SEQ"""

#Checking the shapes of every element we will use in our seq2seq model
print(decoder_final_output.shape, decoder_input_data.shape, encoder_input_data.shape,word_to_index['sos'])

enc_inputs = Input(shape=(17,))
enc_embedding = Embedding(15000, 100, mask_zero=True)(enc_inputs)
enc_outputs, state_h, state_c = LSTM(512, return_state=True)(enc_embedding)
enc_states = [state_h, state_c]

# decoder will be used to capture  relations 
# between words from the answers using encoder's 
# internal state as a context
dec_inputs = Input(shape=(17,))
dec_embedding = Embedding(15000, 100, mask_zero=True)(dec_inputs)
dec_lstm = LSTM(512, return_state=True, return_sequences=True)
dec_outputs, _, _ = dec_lstm(dec_embedding,initial_state=enc_states)

# decoder is connected to the output Dense layer
dec_dense = Dense(15000, activation='softmax')
output = dec_dense(dec_outputs)

model = Model([enc_inputs, dec_inputs], output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model.fit([encoder_input_data, decoder_input_data],decoder_final_output,batch_size=50,epochs=50)

#Enc_model for inference, predicting the states for the decoder 
enc_model = Model(inputs=enc_inputs, outputs=enc_states)
#Creating decoder_model for sentences.
dec_state_input_h = Input(shape=(512,))
dec_state_input_c = Input(shape=(512,))
dec_states_inputs = [dec_state_input_h, dec_state_input_c]

dec_outputs, state_h, state_c = dec_lstm(dec_embedding,initial_state=dec_states_inputs)
dec_states = [state_h, state_c]

dec_outputs = dec_dense(dec_outputs)
dec_model = Model(inputs=[dec_inputs] + dec_states_inputs,outputs=[dec_outputs] + dec_states)

model.save('/content/drive/MyDrive/final_model.h5')
enc_model.save('/content/drive/MyDrive/enc_model.h5')
dec_model.save('/content/drive/MyDrive/dec_model.h5')

#Function that converts our string into the appropriate form for our models
def str_to_tokens(sentence: str):
    # convert input string to lowercase, 
    # then split it by whitespaces
    words = sentence.lower().split()
    # and then convert to a sequence 
    # of integers padded with zeros
    tokens_list = list()
    for current_word in words:
        result = tokenizer.word_index.get(current_word, '')
        if result != '':
            tokens_list.append(result)
    return pad_sequences([tokens_list],maxlen=17,padding='post')

model=tf.keras.models.load_model('/content/drive/MyDrive/final_model.h5')
enc_model=tf.keras.models.load_model('/content/drive/MyDrive/enc_model.h5')
dec_model=tf.keras.models.load_model('/content/drive/MyDrive/dec_model.h5')

user_text=''
print('---------------------------')
print('ENTERING CHAT')
print('Press "/q" to quit chatting')
print('---------------------------')
while user_text!='/q':
  user_text=input('You: ')
  if user_text=='/q':
    break
  else:
    states_values=enc_model.predict(str_to_tokens(user_text),verbose=0)
    # start with a target sequence of size 1 - word 'sos'   
    empty_target_seq = np.zeros((1, 1))
    empty_target_seq[0, 0] = tokenizer.word_index['sos']
    stop_condition = False
    decoded_translation = ''
    while not stop_condition:
        # feed the state vectors and 1-word target sequence 
        # to the decoder to produce predictions for the next word
        dec_outputs, h, c = dec_model.predict([empty_target_seq]+ states_values,verbose=0)         
        # sample the next word using these predictions
        sampled_word_index = np.argmax(dec_outputs[:, -1, :])
        sampled_word = None
        # append the sampled word to the target sequence
        for word, index in word_to_index.items():
            if sampled_word_index == index:
                if word != 'eos':
                    decoded_translation += ' {}'.format(word)
                sampled_word = word
        # repeat until we generate the end-of-sequence word 'eos' 
        # or we hit the length of answer limit
        if sampled_word == 'eos'  or len(decoded_translation.split())>17:
            stop_condition = True
        # prepare next iteration
        empty_target_seq = np.zeros((1, 1))
        empty_target_seq[0, 0] = sampled_word_index
        states_values = [h, c]
    print('Bot: ' + str(decoded_translation))

predicted_answers=[]
i=0
for question in evaluating_questions:
  states_values=enc_model.predict(str_to_tokens(question),verbose=0)
  # start with a target sequence of size 1 - word 'start'   
  empty_target_seq = np.zeros((1, 1))
  empty_target_seq[0, 0] = tokenizer.word_index['sos']
  stop_condition = False
  decoded_translation = ''
  while not stop_condition:
      # feed the state vectors and 1-word target sequence 
      # to the decoder to produce predictions for the next word
      dec_outputs, h, c = dec_model.predict([empty_target_seq]+ states_values,verbose=0)         
      # sample the next word using these predictions
      sampled_word_index = np.argmax(dec_outputs[:, -1, :])
      sampled_word = None
      # append the sampled word to the target sequence
      for word, index in word_to_index.items():
          if sampled_word_index == index:
              if word != 'eos':
                  decoded_translation += ' {}'.format(word)
              sampled_word = word
      # repeat until we generate the end-of-sequence word 'end' 
      # or we hit the length of answer limit
      if sampled_word == 'eos'  or len(decoded_translation.split())>17:
          stop_condition = True
      
      # prepare next iteration
      empty_target_seq = np.zeros((1, 1))
      empty_target_seq[0, 0] = sampled_word_index
      states_values = [h, c]
  predicted_answers.append(decoded_translation)
  print(i)
  i+=1

def jaccard_similarity(x,y):
  """ returns the jaccard similarity between two lists """
  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))
  union_cardinality = len(set.union(*[set(x), set(y)]))
  return intersection_cardinality/float(union_cardinality)

evaluation_tokenizer = Tokenizer()

# fit the tokenizer on the responses and answers
evaluation_tokenizer.fit_on_texts(predicted_answers + evaluating_answers)
evaluating_answers_vectors = evaluation_tokenizer.texts_to_sequences(evaluating_answers)
predicted_answers_vectors = evaluation_tokenizer.texts_to_sequences(predicted_answers)

jaccard_similarities=[]
for i in range(len(evaluating_answers)):
  jaccard_similarities.append(jaccard_similarity(evaluating_answers_vectors[i],predicted_answers_vectors[i]))

print(np.mean(jaccard_similarities))